{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get, post\n",
    "import urllib.parse as up\n",
    "import urllib\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "def encode_and_replace(li):\n",
    "    li.encode('UTF-8')\n",
    "    return li.attrs['href'] if 'href' in li.attrs else ''\n",
    "\n",
    "class page(object):\n",
    "    def getHiperlinks(self):\n",
    "        query='banana'\n",
    "        query_template = 'https://uk.wikipedia.org?{}'\n",
    "        url = query_template.format(up.urlencode({'q': query}))\n",
    "        if self.title =='https://uk.wikipedia.org':\n",
    "            url = 'https://uk.wikipedia.org'\n",
    "        else: url = 'https://uk.wikipedia.org' + self.title\n",
    "        \n",
    "        req = get(url)\n",
    "        soup = BeautifulSoup(req.content.decode(req.encoding), \"lxml\")\n",
    "        \n",
    "        arList = [encode_and_replace(li) for li in soup.findAll('a')]\n",
    "        arList = list(set([i for i in arList if self.validation(i)]))\n",
    "        return arList\n",
    "    \n",
    "    def __init__(self, title):\n",
    "        self.title=title\n",
    "        self.pageList=self.getHiperlinks()\n",
    "    \n",
    "    def __str__(self):\n",
    "        s = \"(\" + str(self.title) + \":\\n  \"\n",
    "        for i in self.pageList:\n",
    "            s += str(i) + \";  \"\n",
    "        s += ')'\n",
    "        return s\n",
    "\n",
    "    def validation(self, ar):\n",
    "        bad = min(list(map(lambda x: not x in ar, features_not)))\n",
    "        return bad\n",
    "\n",
    "features_not = ['Special', '.wikipedia.org', \n",
    "                'wikidata', 'wiktionary', 'wikibooks', \n",
    "                'wikiquote', 'en.wiki',\n",
    "                'wikinews', 'meta.w', 'mediawiki', '/w/']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from copy import deepcopy\n",
    "class crawler(object):\n",
    "    def __init__(self, init, n = 3, batch = 250):\n",
    "        self.data=[]\n",
    "        self.unvisitedPage=[]\n",
    "        self.unvisitedPage.append(init)\n",
    "        self.n = n\n",
    "        self.workers = 0\n",
    "        self.newSet = []\n",
    "        self.batch = batch\n",
    "        \n",
    "    def __contains__(self, value):\n",
    "        result = [i.title == value for i in self.data]\n",
    "        return max(result) if result else False\n",
    "        \n",
    "    def add(self, page):\n",
    "        self.data.append(page)\n",
    "        \n",
    "    def __str__(self):\n",
    "        s = \"Data:\\n\"\n",
    "        for i in self.data:\n",
    "            s += str(i) + \"\\n\"\n",
    "        s += \"unvisited pages:\\n\"\n",
    "        for i in self.unvisitedPage:\n",
    "            s += str(i) + \"\\n\"\n",
    "        return s\n",
    "    \n",
    "    def one_hand(self, p):\n",
    "        page_i = page(p)\n",
    "        if not p in self:\n",
    "            self.add(page_i)\n",
    "        self.newSet += [i for i in page_i.pageList if 'http' not in i and 'irc' not in i] \n",
    "        self.workers -= 1\n",
    "    \n",
    "    def toCraw(self):\n",
    "        self.newSet = []\n",
    "        for i in self.unvisitedPage[:self.batch]:\n",
    "            while not self.has_free_hands():\n",
    "                sleep(0.2)\n",
    "            self.workers += 1\n",
    "            Thread(target = self.one_hand, args = (i,)).start()\n",
    "        b = time()\n",
    "        #print('workers', self.workers, ' ')\n",
    "        while(self.workers > 0):\n",
    "            sleep(0.2)\n",
    "        #print('waiting:', time() - b)\n",
    "        \n",
    "        self.x = 0\n",
    "        self.newSet = list(set(self.newSet) - set([i.title for i in self.data]))\n",
    "        self.unvisitedPage = self.unvisitedPage[self.batch:] + self.newSet\n",
    "        self.newSet = []\n",
    "\n",
    "    \n",
    "    def has_step(self):\n",
    "        return len(self.unvisitedPage) > 0\n",
    "        \n",
    "        \n",
    "    def has_free_hands(self):\n",
    "        return self.n > self.workers\n",
    "    \n",
    "    def filter_url_img(self, uns = False):\n",
    "        img = ['.img', '.jpg', '.bmp', '.png', '.svg']\n",
    "        res = []\n",
    "        if not uns:\n",
    "            for i in self.data:\n",
    "                if  max(map(lambda x: x in i.title, img)) and '/wiki/' not in i.title:\n",
    "                    res.append(i.title)\n",
    "            return res\n",
    "        else:\n",
    "            return self.unvisitedPage\n",
    "    \n",
    "    def download_one_img(self, i, img):\n",
    "        try:\n",
    "            with urllib.request.urlopen('https:' + img) as url:\n",
    "                s = url.read()\n",
    "            t = img.split('.')[-1]\n",
    "            out = open('images/' + str(i) + '.' + t, 'wb')\n",
    "            out.write(s)\n",
    "            out.close()\n",
    "            self.workers -= 1\n",
    "        except:\n",
    "            self.workers -= 1\n",
    "            \n",
    "    def download_img(self, uns=False):\n",
    "        res = self.filter_url_img(uns = uns)\n",
    "        for i, img in enumerate(res):\n",
    "            while(self.workers >= self.n):\n",
    "                sleep(0.2)\n",
    "            self.workers += 1\n",
    "            Thread(target = self.download_one_img, args = (i, img)).start()\n",
    "        print('uploading done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "unvisited pages:\n",
      "https://uk.wikipedia.org\n",
      "\n",
      "visited 1 unvisited 390\n",
      "visited 251 unvisited 32111\n",
      "visited 501 unvisited 71572\n",
      "visited 751 unvisited 93214\n",
      "visited 999 unvisited 122335\n",
      "visited 1249 unvisited 146620\n",
      "visited 1498 unvisited 173835\n",
      "visited 1747 unvisited 198380\n",
      "visited 1991 unvisited 224969\n",
      "visited 2241 unvisited 250961\n",
      "visited 2490 unvisited 273666\n",
      "visited 2740 unvisited 295890\n",
      "visited 2989 unvisited 320898\n",
      "visited 3239 unvisited 347075\n",
      "visited 3486 unvisited 373838\n",
      "visited 3736 unvisited 397703\n",
      "visited 3985 unvisited 421988\n",
      "visited 4235 unvisited 445127\n",
      "visited 4485 unvisited 467170\n",
      "visited 4735 unvisited 489142\n",
      "visited 4985 unvisited 512998\n",
      "visited 5235 unvisited 536106\n",
      "visited 5480 unvisited 558657\n",
      "visited 5730 unvisited 582979\n",
      "visited 5977 unvisited 608650\n",
      "visited 6227 unvisited 629687\n",
      "visited 6475 unvisited 655063\n",
      "visited 6725 unvisited 677543\n",
      "visited 6970 unvisited 701351\n",
      "visited 7218 unvisited 726271\n",
      "visited 7466 unvisited 749701\n",
      "visited 7713 unvisited 774456\n",
      "visited 7962 unvisited 796603\n",
      "visited 8211 unvisited 820446\n",
      "visited 8460 unvisited 846476\n",
      "visited 8710 unvisited 871163\n",
      "visited 8955 unvisited 893282\n",
      "visited 9205 unvisited 918395\n",
      "visited 9452 unvisited 946177\n",
      "visited 9702 unvisited 968392\n",
      "visited 9952 unvisited 992122\n",
      "visited 10200 unvisited 1016074\n",
      "visited 10450 unvisited 1040002\n",
      "visited 10699 unvisited 1061729\n",
      "visited 10948 unvisited 1083030\n",
      "visited 11197 unvisited 1103829\n",
      "visited 11445 unvisited 1127820\n",
      "visited 11694 unvisited 1147598\n",
      "visited 11942 unvisited 1169959\n",
      "visited 12192 unvisited 1192592\n",
      "visited 12441 unvisited 1212680\n",
      "visited 12691 unvisited 1234554\n",
      "visited 12939 unvisited 1256595\n",
      "visited 13189 unvisited 1278120\n",
      "visited 13432 unvisited 1301247\n",
      "visited 13682 unvisited 1324327\n",
      "visited 13929 unvisited 1350620\n",
      "visited 14179 unvisited 1372487\n",
      "visited 14427 unvisited 1397399\n",
      "visited 14677 unvisited 1414452\n",
      "visited 14925 unvisited 1435895\n",
      "visited 15175 unvisited 1459312\n",
      "visited 15423 unvisited 1478625\n",
      "visited 15673 unvisited 1504277\n",
      "visited 15923 unvisited 1524883\n",
      "visited 16173 unvisited 1545993\n",
      "visited 16422 unvisited 1567697\n",
      "visited 16672 unvisited 1589161\n",
      "visited 16922 unvisited 1612250\n",
      "visited 17172 unvisited 1632898\n",
      "visited 17422 unvisited 1651437\n",
      "visited 17672 unvisited 1674866\n",
      "visited 17920 unvisited 1695246\n",
      "visited 18170 unvisited 1717040\n",
      "visited 18416 unvisited 1736758\n",
      "visited 18666 unvisited 1758687\n",
      "visited 18915 unvisited 1778673\n",
      "visited 19165 unvisited 1798200\n",
      "visited 19413 unvisited 1820338\n",
      "visited 19663 unvisited 1839048\n",
      "visited 19910 unvisited 1861117\n",
      "visited 20160 unvisited 1884236\n",
      "visited 20409 unvisited 1907026\n",
      "visited 20659 unvisited 1926735\n",
      "visited 20906 unvisited 1947967\n",
      "visited 21156 unvisited 1965289\n",
      "visited 21406 unvisited 1984413\n",
      "visited 21656 unvisited 2005276\n",
      "visited 21905 unvisited 2027833\n",
      "visited 22155 unvisited 2047839\n",
      "visited 22402 unvisited 2070895\n",
      "visited 22652 unvisited 2090420\n",
      "visited 22901 unvisited 2110071\n",
      "visited 23151 unvisited 2130833\n",
      "visited 23400 unvisited 2151550\n",
      "visited 23650 unvisited 2171048\n",
      "visited 23899 unvisited 2190258\n",
      "visited 24149 unvisited 2209962\n",
      "visited 24398 unvisited 2226905\n",
      "visited 24648 unvisited 2245877\n",
      "visited 24894 unvisited 2265548\n",
      "visited 25144 unvisited 2286176\n",
      "visited 25392 unvisited 2302747\n",
      "visited 25642 unvisited 2318969\n",
      "visited 25891 unvisited 2337204\n",
      "visited 26141 unvisited 2356617\n",
      "visited 26389 unvisited 2375755\n",
      "visited 26639 unvisited 2392742\n",
      "visited 26887 unvisited 2410170\n",
      "visited 27137 unvisited 2430144\n",
      "visited 27386 unvisited 2449215\n",
      "visited 27636 unvisited 2470029\n",
      "visited 27883 unvisited 2490308\n",
      "visited 28133 unvisited 2509209\n",
      "visited 28382 unvisited 2524861\n",
      "visited 28632 unvisited 2542862\n",
      "visited 28877 unvisited 2560018\n",
      "visited 29127 unvisited 2576990\n",
      "visited 29374 unvisited 2595525\n",
      "visited 29624 unvisited 2614554\n",
      "visited 29873 unvisited 2629506\n",
      "visited 30122 unvisited 2648691\n",
      "visited 30369 unvisited 2666326\n",
      "visited 30619 unvisited 2682103\n",
      "visited 30868 unvisited 2700306\n",
      "visited 31118 unvisited 2723137\n",
      "visited 31364 unvisited 2740861\n",
      "visited 31614 unvisited 2757804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-71514:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connection.py\", line 138, in _new_conn\n",
      "    (self.host, self.port), self.timeout, **extra_kw)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\util\\connection.py\", line 75, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\socket.py\", line 743, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 594, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 350, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 835, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connection.py\", line 281, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connection.py\", line 147, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e)\n",
      "requests.packages.urllib3.exceptions.NewConnectionError: <requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x0000015B39AAC5F8>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 423, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 643, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\util\\retry.py\", line 363, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "requests.packages.urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='uk.wikipedia.orgftp', port=443): Max retries exceeded with url: //ftp.gnome.org/pub/gnome/sources/adwaita-icon-theme/ (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x0000015B39AAC5F8>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-8-b51731ad8eea>\", line 30, in one_hand\n",
      "    page_i = page(p)\n",
      "  File \"<ipython-input-7-23a4244fd0eb>\", line 32, in __init__\n",
      "    self.pageList=self.getHiperlinks()\n",
      "  File \"<ipython-input-7-23a4244fd0eb>\", line 23, in getHiperlinks\n",
      "    req = get(url)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\", line 70, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\", line 56, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 488, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 609, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 487, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='uk.wikipedia.orgftp', port=443): Max retries exceeded with url: //ftp.gnome.org/pub/gnome/sources/adwaita-icon-theme/ (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x0000015B39AAC5F8>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',))\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-dc3c38caa589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mcr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoCraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'visited'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'unvisited'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munvisitedPage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'threads: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'time: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b51731ad8eea>\u001b[0m in \u001b[0;36mtoCraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munvisitedPage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_free_hands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkers\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mThread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in [16]:\n",
    "    total = time()\n",
    "    moWiki = 'https://uk.wikipedia.org'\n",
    "    cr = crawler(moWiki, n = i)\n",
    "    print(cr)\n",
    "    while(cr.has_step()):\n",
    "        b = time()\n",
    "        cr.toCraw()\n",
    "        print('visited', len(cr.data), 'unvisited', len(cr.unvisitedPage))\n",
    "    print('threads: ', i, 'time: ',time() - total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "print(len(cr.filter_url_img()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-334868ae63fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-b51731ad8eea>\u001b[0m in \u001b[0;36mdownload_img\u001b[0;34m(self, uns)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkers\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mThread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_one_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'uploading done!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0m_limbo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_started\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cr.download_img(uns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "moWiki = 'https://mo.wikipedia.org'\n",
    "lis = ['https://mo.wikipedia.org']\n",
    "for pages in cr.data:\n",
    "    lis+= pages.pageList\n",
    "lis = list(set(lis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = {}\n",
    "for i,k in enumerate(lis):\n",
    "    index.update({k:i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix = []\n",
    "zero=[]\n",
    "x=0\n",
    "for i in range(0,len(cr.data)):\n",
    "    zero=[]\n",
    "    for i in range(0,len(cr.data)):\n",
    "        zero.append(0)\n",
    "    matrix.append(zero)\n",
    "for i in cr.data:\n",
    "    for j in i.pageList:\n",
    "        matrix[index[i.title]][index[j]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gr = matrix\n",
    "#print(gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "11\n",
      "45\n",
      "148\n",
      "348\n",
      "421\n",
      "429\n",
      "[1, 2, 4, 4, 3, 3, 3, 2, 4, 5, 5, 3, 4, 4, 4, 3, 4, 4, 5, 5, 3, 4, 4, 4, 3, 2, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 2, 5, 4, 3, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 3, 3, 4, 4, 4, 4, 2, 5, 4, 4, 4, 3, 4, 3, 3, 5, 1, 3, 5, 5, 5, 4, 5, 3, 3, 4, 6, 5, 3, 4, 4, 4, 3, 5, 4, 4, 4, 5, 3, 2, 4, 4, 6, 2, 4, 3, 4, 3, 4, 5, 4, 4, 4, 3, 4, 4, 5, 5, 3, 5, 3, 3, 3, 3, 7, 1, 2, 3, 5, 5, 5, 5, 2, 3, 4, 2, 4, 5, 4, 4, 5, 4, 4, 5, 4, 3, 7, 3, 5, 4, 2, 3, 5, 3, 3, 4, 5, 4, 4, 4, 2, 5, 3, 4, 3, 4, 4, 4, 3, 4, 5, 5, 2, 4, 4, 2, 3, 4, 3, 3, 3, 4, 4, 3, 5, 0, 4, 6, 2, 5, 3, 4, 4, 4, 3, 1, 3, 4, 3, 4, 3, 3, 5, 4, 5, 3, 5, 5, 3, 4, 4, 4, 3, 2, 4, 4, 4, 3, 3, 4, 2, 4, 4, 4, 4, 4, 3, 4, 3, 4, 5, 4, 2, 3, 3, 4, 2, 4, 4, 5, 4, 3, 4, 4, 0, 3, 3, 4, 4, 4, 3, 4, 3, 5, 6, 2, 3, 5, 3, 3, 6, 2, 3, 2, 3, 3, 3, 3, 4, 2, 5, 4, 5, 1, 4, 6, 3, 1, 2, 3, 6, 1, 4, 5, 4, 4, 5, 2, 4, 3, 4, 1, 5, 4, 5, 4, 4, 5, 4, 2, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 4, 5, 4, 4, 5, 4, 4, 5, 4, 5, 4, 4, 3, 4, 4, 4, 4, 4, 3, 6, 3, 4, 2, 4, 2, 5, 5, 3, 3, 4, 4, 4, 4, 3, 4, 4, 5, 5, 3, 3, 4, 4, 2, 2, 4, 4, 3, 3, 3, 5, 3, 4, 4, 5, 4, 3, 5, 4, 4, 4, 5, 4, 4, 4, 4, 2, 5, 1, 4, 4, 3, 4, 3, 4, 3, 4, 2, 4, 4, 5, 3, 4, 4, 4, 5, 3, 4, 2, 5, 1, 4, 4, 3, 3, 4, 4, 4, 4, 4, 3, 4, 3, 2, 4, 5, 4, 5, 3, 4, 4, 4, 3, 4, 4, 3, 3, 4, 5, 3, 4, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "dist=[]\n",
    "for i in range(0,len(gr[0])):\n",
    "        dist.append(0)\n",
    "x=0\n",
    "poss=[240]      \n",
    "visited=set()\n",
    "curr=set()\n",
    "while len(visited)!=len(gr)-1:\n",
    "    x+=1\n",
    "    print(len(visited))\n",
    "    for k in list(poss):\n",
    "        for i in range(0,len(gr[k])):\n",
    "            if gr[k][i]==1 and i!=240 and i not in visited:\n",
    "                #print(i)\n",
    "                dist[i]=x\n",
    "                visited.update({i})\n",
    "                curr.update({i})\n",
    "                visited.update({k})\n",
    "    poss=set(curr)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
